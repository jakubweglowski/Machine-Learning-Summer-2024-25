{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2eac31",
   "metadata": {},
   "source": [
    "# Understanding Transposed Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e1f60",
   "metadata": {},
   "source": [
    "## Theory Exploration\n",
    "\n",
    "**What is a transposed convolution?**\n",
    "\n",
    "A *transposed convolution*, also known as a *fractionally-strided convolution* or (misleadingly) a *deconvolution*, is a type of convolutional layer that performs an upsampling operation. Instead of mapping multiple input features to a single output feature (like a regular convolution), it associates a single input feature with multiple output features. It's essentially learning how to reverse a standard convolution operation in terms of spatial dimensions, aiming to reconstruct a higher-resolution feature map from a lower-resolution one.\n",
    "\n",
    "While it's often called *deconvolution*, this term is mathematically inaccurate. A true deconvolution would perfectly reverse the convolution operation, recovering the original input. Transposed convolutions, however, learn to perform an upsampling that resembles a reversal but isn't a perfect inverse. They learn the appropriate weights to generate a useful, larger feature map.\n",
    "\n",
    "**How does it differ from a regular convolution?**\n",
    "\n",
    "The core difference lies in their primary function and how they map inputs to outputs spatially:\n",
    "\n",
    "Regular Convolution (Downsampling):\n",
    "\n",
    "- Purpose: Feature extraction, dimensionality reduction (spatial).\n",
    "- Process: A kernel slides over an input feature map. At each position, it performs an element-wise multiplication with the overlapping input region and sums the results to produce a single output value.\n",
    "Input-to-Output Mapping: Many input values contribute to one output value. This typically leads to an output feature map that is smaller than or the same size as the input (unless significant padding is used).\n",
    "\n",
    "Transposed Convolution (Upsampling):\n",
    "\n",
    "- Purpose: Upsampling, increasing spatial resolution.\n",
    "- Process: Conceptually, it can be thought of as \"projecting\" each input pixel onto a larger output region. Each input pixel's value is multiplied by the kernel weights, and the results are \"painted\" onto an output grid. Where these projections overlap, the values are summed.\n",
    "\n",
    "Input-to-Output Mapping: \n",
    "\n",
    "One input value contributes to many output values (a region in the output feature map defined by the kernel size). This typically leads to an output feature map that is larger than the input.\n",
    "A key insight is that the forward pass of a transposed convolution is equivalent to the backward pass (gradient computation) of a regular convolution, and vice-versa. This is where the name \"transposed\" comes from – the operation can be represented by a transformation matrix that is the transpose of the matrix representing the equivalent regular convolution.\n",
    "\n",
    "**How does it upsample feature maps?**\n",
    "\n",
    "Transposed convolution upsamples feature maps by essentially reversing the connectivity of a standard convolution. Imagine a single pixel in a low-resolution input feature map. In a transposed convolution:\n",
    "\n",
    "- *Scalar Multiplication*: This single input pixel value is multiplied by all the weights in the transposed convolution kernel, creating a small feature map (the size of the kernel).\n",
    "- *Placement/Projection*: This resulting kernel-sized feature map is then placed onto a larger output grid. The position where it's placed depends on the stride.\n",
    "- *Aggregation*: If multiple such projections (from different input pixels) overlap on the output grid, their values are summed up at the overlapping locations.\n",
    "\n",
    "The stride plays a crucial role in upsampling. A stride greater than 1 effectively \"stretches\" the input, inserting zeros between the input pixels before the kernel operation, which then fills in these expanded areas.\n",
    "\n",
    "**What are stride, padding, and kernel size, and how do they influence the result in a transposed convolution?**\n",
    "These parameters control the geometry and output size of the transposed convolution:\n",
    "\n",
    "1. Kernel Size ($k$ or $k_h\\times k_w$):\n",
    "\n",
    "By definition, kernel size gives the dimensions (height and width) of the filter that is used.\n",
    "\n",
    "In a transposed convolution, each input pixel value is multiplied by the entire kernel. The kernel's values are learned during training. The size of the kernel determines the local neighborhood in the output that a single input pixel can influence. A larger kernel means a single input pixel can directly affect a larger area in the output.\n",
    "\n",
    "2. Stride ($s$ or $s_h\\times s_w$):\n",
    "\n",
    "By definition, stride is the step size by which the kernel is moved across the output grid (or, equivalently, how far apart input pixel influences are \"placed\" on the output grid). This is different from a regular convolution where stride dictates movement over the input grid. Stride is the primary mechanism for upsampling.\n",
    "\n",
    "A stride of $s=1$ means the kernel (scaled by each input pixel) is placed at positions one pixel apart on the output grid. This usually results in significant overlap.\n",
    "\n",
    "A stride of $s>1$ means the kernel applications are placed s pixels apart. This creates a larger output. Conceptually, you can think of the input feature map being expanded by inserting $s−1$ zeros between input pixels (both horizontally and vertically) before applying a convolution with a unit stride (stride=$1$) using the transposed convolution kernel. The kernel then \"fills in\" these gaps.\n",
    "\n",
    "3. Padding ($p$ or $p_h\\times p_w$):\n",
    "\n",
    " In a regular convolution, padding adds zeros around the input to control the output size. In transposed convolutions, padding effectively removes pixels from the output. This is a bit counter-intuitive notion for transposed convolutions.\n",
    "\n",
    "Padding in a transposed convolution determines how much of the \"full\" convolved output (after the kernel is applied and potentially overlaps) is discarded from the edges. If you imagine a convolution that would produce a certain output size $O_{full}$, padding $p$ means the final output $O$ will be $O_{full}−2p$. It essentially crops the output of what would otherwise be a larger convolution.\n",
    "\n",
    "Padding is often used to fine-tune the output dimensions to match a target size, especially in architectures like U-Net where feature maps from the encoder are concatenated with upsampled feature maps from the decoder.\n",
    "Relationship between Input Size ($I$), Output Size ($O$), Kernel Size ($k$), Stride ($s$), and Padding ($p$):\n",
    "\n",
    "For a $1D$ transposed convolution (the $2D$ case is analogous for height and width independently):\n",
    "\n",
    "The relationship for output size $O$ given input size $I$, kernel size $k$, stride $s$, and padding $p$ is:\n",
    "\n",
    "$O=s(I−1)+k−2p$\n",
    "\n",
    "Let's understand why:\n",
    "\n",
    "$I−1$: This is the number of \"steps\" or intervals between the $I$ input pixels.\n",
    "\n",
    "$s(I−1)$: The stride value scales these intervals. If you place the first kernel application corresponding to the first input pixel, and then move s pixels for each subsequent input pixel, this term gives the distance from the start of the first kernel application to the start of the last kernel application based on the input pixels.\n",
    "\n",
    "$+k$: The kernel itself has a width $k$. So, after placing the kernel based on the last input pixel, it extends for $k$ pixels.\n",
    "\n",
    "$−2p$: The padding $p$ then removes p pixels from each side of this \"full\" or \"effective\" convolved output.\n",
    "\n",
    "**Example Step-by-Step Upsampling Process**:\n",
    "\n",
    "Let's consider a simple $1D$ example:\n",
    "\n",
    "Input ($I$): $[x1, x2]$ (Size = $2$)\n",
    "Kernel ($k$): $[w1, w2, w3]$ (Size = $3$)\n",
    "Stride ($s$): $2$\n",
    "Padding ($p$): $1$\n",
    "\n",
    "**Conceptual Input Expansion (due to stride):**\n",
    "If we imagine expanding the input by inserting $s−1=2−1=1$ zeros between input elements, the effective input becomes $[x1, 0, x2]$. This isn't strictly how it's implemented, but it helps understand the stride's effect.\n",
    "\n",
    "**Kernel Application (No Padding First, then Crop):**\n",
    "\n",
    "Take the first input pixel $x1$. Multiply it by the kernel: $[x1*w1, x1*w2, x1*w3]$.\n",
    "Take the second input pixel $x2$. Multiply it by the kernel: $[x2*w1, x2*w2, x2*w3]$.\n",
    "\n",
    "The result for $x1$ is placed starting at index $0$ of an intermediate output grid.\n",
    "The result for $x2$ is placed starting at index $0+s=2$ of the intermediate output grid.\n",
    "\n",
    "The size of this full convolved output before padding adjustment is $s⋅(I−1)+k=2⋅(2−1)+3=2⋅1+3=5$.\n",
    "Values: $[x1*w1, x1*w2, x1*w3 + x2*w1, x2*w2, x2*w3]$ (if stride allowed overlap as in a regular convolution, but here stride determines placement of the start of kernel ops).\n",
    "\n",
    "Let's refine the \"placement\" for upsampling. Each input element effectively contributes to an output region.\n",
    "\n",
    "Input $x1$ contributes $x1*w1$, $x1*w2$, $x1*w3$ to output positions $0, 1, 2$.\n",
    "Input $x2$ contributes $x2*w1$, $x2*w2$, $x2*w3$ to output positions $0+s, 1+s, 2+s$, here: $2, 3, 4$.\n",
    "\n",
    "The padding $p=1$ means we remove $1$ element from the beginning and $1$ from the end of this $5$-element intermediate result.\n",
    "Output after padding: $[x1*w2, x1*w3 + x2*w1, x2*w2]$\n",
    "\n",
    "The output size is $O=s⋅(I−1)+k−2p=2⋅(2−1)+3−2⋅1=2+3−2=3$.\n",
    "This matches our manually derived output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327598d",
   "metadata": {},
   "source": [
    "Now let's consider a $2D$ grid:\n",
    "\n",
    "Input:\n",
    "$$I = \\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}$$\n",
    "\n",
    "And a 2x2 kernel:\n",
    "$$K = \\begin{pmatrix}w11 & w12 \\\\ w21 & w22\\end{pmatrix}$$\n",
    "\n",
    "Let's set stride $s=2$ and padding $p=0$.\n",
    "\n",
    "Output size $O_h = s(I_h−1)+k_h−2p=2⋅(2−1)+2−0=2⋅1+2=4$. This means that we have a $4\\times 4$ output.\n",
    "\n",
    "Element-wise Kernel Multiplication and Placement:\n",
    "\n",
    "Element $a$ (placed at top-left of output): \n",
    "$$aK=\\begin{pmatrix}aw11 & aw12 \\\\ aw21 & aw22\\end{pmatrix}$$ \n",
    "\n",
    "Element $b$ (Placed starting at column $0+s=2$, row $0$): \n",
    "$$bK=\\begin{pmatrix}bw11 & bw12 \\\\ bw21 & bw22\\end{pmatrix}$$\n",
    "\n",
    "Element $c$ (Placed starting at column $0$, row $0+s=2$): \n",
    "$$cK=\\begin{pmatrix}cw11 & cw12 \\\\ cw21 & cw22\\end{pmatrix}$$\n",
    "\n",
    "Element $d$ (Placed starting at column $0+s=2$, row $0+s=2$): \n",
    "$$dK=\\begin{pmatrix}dw11 & dw12 \\\\ dw21 & dw22\\end{pmatrix}$$\n",
    "\n",
    "**Output Grid Construction (Summing if overlaps):**\n",
    "The output grid would be formed by summing these contributions. With $s=2$ and $k=2$, there's no overlap between the contributions of adjacent input pixels in this specific case.\n",
    "\n",
    "$$ O = \\begin{pmatrix}\n",
    "aw_{11} & aw_{12} & bw_{11} & bw_{12} \\\\\n",
    "aw_{21} & aw_{22} & bw_{21} & bw_{22} \\\\\n",
    "cw_{11} & cw_{12} & dw_{11} & dw_{12} \\\\\n",
    "cw_{21} & cw_{22} & dw_{21} & dw_{22}\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "If, for example, the stride was $s=1$ (still with $k=2$, $p=0$), we would have\n",
    "$O_h =1\\cdot(2−1)+2−0=1+2=3$. Output would be $3\\times 3$.\n",
    "\n",
    "- $a$ contributes to output region $(0:1, 0:1)$\n",
    "- $b$ contributes to output region $(0:1, 1:2)$\n",
    "- $c$ contributes to output region $(1:2, 0:1)$\n",
    "- $d$ contributes to output region $(1:2, 1:2)$\n",
    "\n",
    "The final output would be:\n",
    "$$ O = \\begin{pmatrix}\n",
    "aw_{11} & aw_{12}+bw_{11} & bw_{12} \\\\\n",
    "aw_{21}+cw_{11} & aw_{22}+bw_{21}+cw_{12}+dw_{11} & bw_{22}+dw_{12} \\\\\n",
    "cw_{21} & cw_{22}+dw_{21} & dw_{22}\n",
    "\\end{pmatrix} $$\n",
    "This shows how overlaps are summed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3978a65",
   "metadata": {},
   "source": [
    "**Reproducing step-by-step**\n",
    "\n",
    "First, determine the output size using the formula. Create an empty output grid of this size, initialized to zeros.\n",
    "Next, for each pixel $(i,j)$ in the input feature map with value $I_{ij}$:\n",
    "​\n",
    "- Multiply the entire kernel $K$ by $I_{ij}$ to get a temporary matrix $M=I_{ij}\\cdot K$. \n",
    "\n",
    "- The top-left corner of this matrix $M$ is placed at position $(i\\cdot s, j\\cdot s)$ in an \"intermediate\" larger canvas (before considering padding for output cropping). \n",
    "\n",
    "- Add the values of $M$ to the corresponding cells in this intermediate canvas.\n",
    "\n",
    "Finally, if padding $p>0$, crop $p$ pixels from all sides (top, bottom, left, right) of the intermediate canvas to get the final output. If padding $p=0$, the intermediate canvas is the final output. (More accurately, the formula $O=s(I−1)+k−2p$ directly gives the final size after padding's effect, and the process involves adding contributions to this final-sized grid, where some contributions might partially fall \"outside\" if we think of a full convolution being cropped).\n",
    "\n",
    "A more common way to think about padding in the context of the formula $O=s(I−1)+k−2p$ is that $p$ refers to the padding that would have been applied to the input of an equivalent direct convolution to achieve the transposed output size. For transposed convolutions, the implementation usually involves convolving the kernel over an input that has been effectively expanded by inserting $s−1$ zeros between its elements, and the 'padding' in the formula adjusts the output cropping."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
